apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "backend.fullname" . }}
  labels:
{{ include "backend.labels" . | indent 4 }}
spec:
  replicas: {{ .Values.replicaCount }}
  selector:
    matchLabels:
      app.kubernetes.io/name: {{ include "backend.name" . }}
      app.kubernetes.io/instance: {{ .Release.Name }}
  template:
    metadata:
      labels:
        app.kubernetes.io/name: {{ include "backend.name" . }}
        app.kubernetes.io/instance: {{ .Release.Name }}
    spec:
    {{- with .Values.hostAliases }}
      hostAliases:
        {{- toYaml . | nindent 8 }}
    {{- end }}
    {{- with .Values.imagePullSecrets }}
      imagePullSecrets:
        {{- toYaml . | nindent 8 }}
    {{- end }}
      serviceAccountName: {{ template "backend.serviceAccountName" . }}
      securityContext:
        {{- toYaml .Values.podSecurityContext | nindent 8 }}
      {{- if and (eq .Values.configmap.ELASTICSEARCH_ENABLED "yes") .Values.enableInitcontainer }}
      initContainers:
        - name: wait-for-elastic-search
          image: busybox
          env:
            - name: ES_HOST
              value: {{ .Values.configmap.ES_HOST | replace "http://" "" | replace ":9200" "" }}

          ## The following script checks if the Elasticsearch service is available,
          ## and if the service is returning a 401 status code (which means it's up and running)
          command:
            - sh
            - -c
            - |
              NAMESPACE=$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace);
              HOST="$ES_HOST.$NAMESPACE.svc.cluster.local"
              until nslookup $HOST > /dev/null 2>&1 && \
                    wget --server-response -qO- http://$HOST:9200/_cluster/health 2>&1 | grep ' 401 '; 
              do
                echo "Waiting for Elasticsearch to be available...";
                sleep 5;
              done;
              echo "Elasticsearch is ready.";
      {{- end }}
      containers:
        - name: {{ .Chart.Name }}
          securityContext:
            {{- toYaml .Values.securityContext | nindent 12 }}
          image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
          imagePullPolicy: {{ .Values.image.pullPolicy }}
          env:
            {{- toYaml .Values.env | nindent 12 }}
          envFrom:
            - configMapRef:
                name: {{ include "backend.fullname" . }}-configmap
          ports:
            - name: http
              containerPort: {{ .Values.service.port }}
              protocol: TCP
          livenessProbe:
            {{- toYaml .Values.livenessProbe | nindent 12 }}
          readinessProbe:
            {{- toYaml .Values.readinessProbe | nindent 12 }}
          resources:
            {{- toYaml .Values.resources | nindent 12 }}
          volumeMounts: 
            - name: config-volume
              mountPath: /home/node/app/functionalAccounts.json
              subPath: functionalAccounts.json
            - name: config-volume
              mountPath: /home/node/app/loggers.json
              subPath: loggers.json
            - name: config-volume
              mountPath: /home/node/app/dist/config/frontend.config.json
              subPath: frontend.config.json
            - name: config-volume
              mountPath: /home/node/app/dist/config/frontend.theme.json
              subPath: frontend.theme.json
      volumes:
        - name: config-volume
          configMap:
            name: {{ include "backend.fullname" . }}-configmap
      {{- with .Values.nodeSelector }}
      nodeSelector:
        {{- toYaml . | nindent 8 }}
      {{- end }}
    {{- with .Values.affinity }}
      affinity:
        {{- toYaml . | nindent 8 }}
    {{- end }}
    {{- with .Values.tolerations }}
      tolerations:
        {{- toYaml . | nindent 8 }}
    {{- end }}
